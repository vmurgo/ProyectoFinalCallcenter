# -*- coding: utf-8 -*-
"""Proyecto_Pre_Entrega_VentasEnero25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ve06XMv_kN-BCZS8XZG3ccmXtT6YdyNs

# **Presentaci√≥n del proyecto**

**Abstracto con Motivaci√≥n y Audiencia**

>




Este proyecto tiene como objetivo analizar y visualizar el desempe√±o de las ventas realizadas por un Call Center de Movistar durante el mes de enero de 2025. A trav√©s de la exploraci√≥n de datos, buscamos identificar tendencias en las ventas, los productos m√°s solicitados y la tasa de activaciones.

Se utilizar√°n t√©cnicas de limpieza de datos, an√°lisis exploratorio y visualizaci√≥n para extraer informaci√≥n valiosa que pueda ayudar en la toma de decisiones estrat√©gicas.

Este an√°lisis est√° dirigido a diferentes actores clave dentro de la empresa, incluyendo:

- Gerencia Comercial ‚Üí Para identificar oportunidades de crecimiento y optimizar estrategias de venta.
- Supervisores del Call Center ‚Üí Para evaluar el rendimiento del equipo y mejorar los procesos de atenci√≥n.
- Analistas de Datos ‚Üí Para generar reportes estrat√©gicos.
- Equipo de Retenci√≥n y Fidelizaci√≥n ‚Üí Para detectar clientes potenciales y reducir la tasa de abandono.

**Contexto Comercial y Anal√≠tico**

El sector de telecomunicaciones es altamente competitivo, y las estrategias de ventas a trav√©s de call centers juegan un papel fundamental en la adquisici√≥n de clientes, fidelizaci√≥n y rentabilidad de las empresas.

**Preguntas/Hip√≥tesis a Resolver mediante el An√°lisis de Datos**

- ¬øCu√°ntas ventas se realizaron en enero de 2025?
- ¬øCu√°l es la tasa de activaci√≥n de las ventas realizadas?
- ¬øCu√°les fueron los productos m√°s vendidos?

Hip√≥tesis:

 - La mayor√≠a de las ventas corresponden a portabilidades en lugar de nuevas l√≠neas
 - El producto m√°s vendido es el m√°s econ√≥mico

**Objetivo**

# **Lectura de datos**

**Librerias necesarias para el analisis de datos**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
!pip install xgboost
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

"""**Configuraci√≥n del entorno de Python en Google Colab**"""

from google.colab import drive
import os

drive.mount("/content/drive")
print(os.getcwd())

os.chdir("/content/drive/My Drive/")
print(os.getcwd())

"""**Lectura del dataset**"""

df = pd.read_csv('/content/drive/MyDrive/DataFrameVentas.csv', sep=";", encoding="latin1")

"""**An√°lisis inicial del dataset**"""

df.shape

"""Indica que mi dataset contiene 91 columnas y 2035 filas"""

df.head()

df.info()

"""**Descripci√≥n de variables**

Id: N√∫mero de id de la carga de venta.

Fecha de venta: fecha en la que se carga la venta

Hora de carga: hora en la que se carga la venta

Fecha de Activaci√≥n: fecha en la que se activa la portabilidad

Linea a portar: n√∫mero de tel√©fono del cliente

Bandeja IRIS: sistema donde fue cargado

Fecha de portaci√≥n: fecha de portabilidad asignada

Fecha de aprobaci√≥n: fecha de aprobacion asignada

Usuario de la linea: n√∫mero del titular de la l√≠nea

Producto anterior: tipo de producto anterior

Plan actual:

Producto: producto que adquiere en esta compa√±ia

Precio:

Cantidad de portas: cantidad de n√∫meros del cliente que realizan la portabilidad

Estado de Solicitud: estado administrativo en el que se encuentra el tr√°mite

Subestado: para alguna aclaraci√≥n necesaria

Motivo de Rechazo: si la solicitud fuese rechazada, se agregan los motivos

Fecha de rechazo: se agrega fecha de dicho rechazo

Motivo de Estado: para informaci√≥n adicional

PO: N√∫mero de PO que asigna Movistar

Nombre y Apellido del titula: Aparece solo el nombre del titular

Apellido: indica el apellido del titular

DNI: n√∫mero de identificaci√≥n del titular

Vendedor: agente que realiz√≥ la venta

ID secundario: ID del agente

Ejecutivo: entidad que realiz√≥ la venta

Supervisor: lider del agente

Envio: medio por el que se realiza el envio

N√∫mero de Guia: n√∫mero de seguimiento

N√∫mero de factura:

Fuente de Solicitud: fuente donde recibe el dato el vendedor

N√∫mero de Orden: aclaraci√≥n entre los administrativos

Observaciones: celda para agregar aclaraciones

Provincia: Provincia del envio

Ciudad: Ciudad del envio

C√≥digo Postal: CP del envio

Fecha de Nacimiento: fecha de nacimiento del titular

Direcci√≥n: direcci√≥n del envio

N√∫mero: n√∫mero de la direcci√≥n del envio

Torre/Monoblock: Torre del envio

Piso: Piso del envio

Departamento: Departamento del envio

Entre Calles: Entre calles del envio

Barrio: Barrio del envio

Manzana: Manzana del envio

Casa/Lote: Casa del envio

Otras referencias: Referencias adicionales

Referencias: Referencias adicionales

Tel√©fono Principal: Tel√©fono principal del titular

Tel√©fono Adicional: Tel√©fono Adicional del titular

Tel√©fono Adicional 2: Tel√©fono Adicional conviviente del titular

Numero de Chip: N√∫mero asignado de CHIP

C√≥digo de area: C√≥digo de √°rea del titular

Prefijo: Prefijo del n√∫mero del titular

Compa√±ia de Tel√©fono: compa√±ia de tel√©fono anterior

Fecha de Cita: fecha retiro en sucursal

Hora de Cita: hora retiro en sucursal

Comentarios: comentarios adicionales

N√∫mero de PIN: c√≥digo de portabilidad asignado

CHIP en mano: indica si el chip fue recibido

Totalizaci√≥n: si agrega FTTH

Fecha de Envio de CHIP: fecha del envio

Fecha entrega SIM: fecha de la entrega

Tipo de confirmaci√≥n SIM: como confirman que llego el chip

Fecha llamado confirmaci√≥n SIM: fecha de confirmaci√≥n

Fecha de Vencimiento de Pin: vencimiento del c√≥digo de portabilidad asignado

Usuario log√≠stica: quien realiza el envio

Comentarios de log√≠stica: comentarios adicionales

D√≠as transcurridos desde: d√≠as transcurridos desde la venta

Fecha de cambio de estado: fecha de √∫ltimo cambio de estado

ANI fijo asignado: N√∫mero de fijo asignado para FTTH

centralizador_ftth_latitud : latitud

centralizador_ftth_longitud longuitud

centralizador_ftth_ctos_cercanos: cantidad de CTOS cercanos

centralizador_fecha_sincronizacion: fecha de centralizado

Formulario de calidad completado: indica si completa formulario

Formulario de calidad exitoso : indica si el formulario esta bien

L√≠nea a portar correcta: campo adicional en el caso de error de carga

Titular de linea correcto : campo adicional en el caso de error de carga

Producto actual correcto: campo adicional en el caso de error de carga

Compa√±√≠a actual correcta: campo adicional en el caso de error de carga

Documento correcto: campo adicional en el caso de error de carga

Domicilio correcta: campo adicional en el caso de error de carga

Producto correcto : campo adicional en el caso de error de carga

Informacion correcta: campo adicional en el caso de error de carga

Email calidad: email de cliente

Interesado en fibra: indica si compr√≥ fibra o no

Direcci√≥n de fibra: direccion de instalaci√≥n de fibra

Link Google Maps: link de la direcci√≥n en google maps
"""

df.describe()

"""# **Data Wranglimg - Limpieza y transformaci√≥n de datos**

**Valores duplicados**
"""

df.duplicated().sum()

"""Identifico que no tengo ningun valor duplicado.

**Valores nulos**
"""

df.isnull().sum()

"""Reviso la cantidad de valores nulos y en que columnas se encuentran."""

df_clear = df.dropna(axis=1, how='all')

"""Despu√©s de revisar los valores, decido eliminar √∫nicamente las columnas que presentan la totalidad de sus valores nulos."""

columnas_a_eliminar = [
    "Id",
    "Formulario de calidad completado", "L√≠nea a portar correcta",
    "Titular de linea correcto", "Producto actual correcto",
    "Compa√±√≠a actual correcta", "Documento correcto",
    "Domicilio correcta", "Producto correcto",
    "Email calidad", "Interesado en fibra",
    "Direcci√≥n de fibra","Link Google Maps",
    "Fecha de env√≠o de Chip", "Fecha entrega SIM",
    "Tipo de confirmaci√≥n SIM","Fecha de Vencimiento de Pin",
    "Usuario log√≠stica","Comentarios de log√≠stica",
    "Fecha de cambio de estado","ANI fijo asignado",
    "Fecha de cita", "Hora de cita",
    "Tel√©fono Principal (Fijo)",
    "Tel√©fono Adicional (Celular)",
    "Tel√©fono Adicional #2 (Celular)",
    "Direcci√≥n", "N√∫mero",
    "Torre / Monoblock","Piso",
    "Departamento","Entre Calles",
    "Barrio","Manzana",
    "Casa / Lote", "Otras referencias",
    "Referencias","Nexo",
    "Fecha de Nacimiento", "Env√≠o",
    "N√∫mero de Gu√≠a","N√∫mero de Orden",
    "Observaciones", "Comentarios",
    "Precio"
]

df_clear = df.drop(columns=columnas_a_eliminar, errors='ignore')

"""Elimino todas las columnas que interpreto que no son √∫tiles para analizar y que entorpecen el an√°lisis de mi dataset."""

print(df_clear.info())

"""**Tratamiento de valores outliers**"""

df_clear["Producto"] = df_clear["Producto"].replace({
    "C. 1.5 GB FULL $19.150 - BONO DE DESCUENTO POR 12 MESES DE $15.700-": "C.Plan 1.5GB",
    "C. 10 GB FULL $57.750 - BONO DE DESCUENTO POR 12 MESES DE $48.900-": "C.Plan 10GB",
    "C. 15 GB FULL $63.950 - BONO DE DESCUENTO POR 12 MESES DE $52.450-": "C.Plan 15GB",
    "C. 25 GB FULL $72.700 - BONO DE DESCUENTO POR 12 MESES DE $59.450-": "C.Plan 25GB",
    "C. 3GB PROMO EXCLUSIVA $20.900 - BONO DE DESCUENTO POR 24 MESES DE $17.800-": "C.Plan 3GB",
    "C. 6GB FULL PROMO EXCLUSIVA $29.800 - BONO DE DESCUENTO POR 24 MESES DE $25.300-": "C.Plan 6GB",
    "FIBRA PLAN MOVISTAR 100 MB": "FIBRA 100MB",
    "FIBRA PLAN MOVISTAR 300 MB": "FIBRA 300MB",
    "FIBRA PLAN MOVISTAR 300 MB + TV DIGITAL": "FIBRA 300MB + TV",
    "FIBRA PLAN MOVISTAR GIGA + TV DIGITAL": "FIBRA 1GB + TV",
    "P. 10 GB FULL $57.750 - BONO DE DESCUENTO POR 12 MESES DE $16.750-": "P.Plan 10GB-",
    "P. 3 GB FULL $25.550 - BONO DE DESCUENTO POR 12 MESES DE $6.550-": "P.Plan 3GB-",
    "C. 3 GB FULL $25.550 - BONO DE DESCUENTO POR 12 MESES DE $21.050-": "C.Plan 3GB",
    "C. 6 GB FULL $29.800 - BONO DE DESCUENTO POR 12 MESES DE $25.300-": "C.Plan 6GB"

})

df_clear["Producto"].value_counts().plot(kind="bar", figsize=(12,6), title="Ventas por Producto")
plt.xlabel("Producto")
plt.ylabel("Cantidad de Ventas")
plt.xticks(rotation=45)
plt.show()

"""El valor del outlier se encuentra en el plan de 3GB, pero decido no eliminarlo o ajustarlo con la moda, ya que representa un caso real y significativo: ventas de productos altamente demandados."""

df_clear[df_clear["Producto"].str.contains("3GB", na=False)]["Estado de Solicitud"].value_counts()

"""Reviso su estado administrativo para saber si realmente son ventas aprobadas. De estas han sido aprobadas 1113 ventas de porta y 4 de corpo. Si bien hay un n√∫mero significativo de ventas en curso o canceladas, considero que este outlier no debe ser eliminado porque representa que es la promo m√°s comprada."""

ventas_por_producto = df_clear["Producto"].value_counts()
plt.figure(figsize=(12,6))
ventas_por_producto.plot(kind="bar", color=['red' if "3GB" in p else 'royalblue' for p in ventas_por_producto.index])
plt.xticks(rotation=45)
plt.xlabel("Producto")
plt.ylabel("Cantidad de Ventas")
plt.title("Distribuci√≥n de Ventas por Producto (Resaltando el Plan 3GB)")
plt.show()

"""Destaco con un color el outlier para marcar el plan m√°s vendido

**Otras transformaciones, normalizaci√≥n, fechas**

# **An√°lisis Exploratorio de datos**

**An√°lisis univariado**
"""

plt.figure(figsize=(8,5))
sns.boxplot(x=df["Cantidad de portas"], color="lightblue")
plt.title("Boxplot de Cantidad de Portas")
plt.show()

"""La mayor parte de los datos se encuentran entre  1 y 3 portas vendidas y su mediana es de 2 porta. El rango intercuart√≠lico indica que la mayoria se encuentran en un rango bajo. Se obsevan varios outliers mayores a 3, pero uno que destaca de 15."""

df_clear[df_clear["Cantidad de portas"] == 15]["Estado de Solicitud"]

"""El outlier no es un error, sino que corresponde a clientes corporativos que portan m√∫ltiples l√≠neas en una sola transacci√≥n. Esto explica el valor at√≠pico en la variable "Cantidad de portas".

**An√°lisis bivariado**

Relaci√≥n entre Tiempo de Activaci√≥n y Cantidad de Portas
"""

sns.scatterplot(x=df_clear["D√≠as transcurridos desde"], y=df_clear["Cantidad de portas"])
plt.title("Relaci√≥n entre Tiempo de Activaci√≥n y Cantidad de Portas")
plt.show()

"""La activaci√≥n suele completarse en per√≠odos de 0 a 10 d√≠as, con algunos casos extendi√©ndose hasta los 25 d√≠as.

Nuevamente aparecen las 15 portas como outlier, pero esta vez en tiempo de activaci√≥n, lo que sugiere que los clientes corporativos tardan m√°s en activar que la porta individuos. Para los dem√°s casos, no hay un patr√≥n claro que indique que m√°s portas = mayor tiempo de activaci√≥n.

**An√°lisis multivariado**
"""

pivot_table = df_clear.pivot_table(index="Vendedor", columns="Producto", values="Cantidad de portas", aggfunc="sum")

pivot_table_top10 = pivot_table.head(10)

plt.figure(figsize=(12,6))
sns.heatmap(pivot_table_top10, cmap="coolwarm", annot=True, fmt=".0f")
plt.title("Matriz de Ventas por Vendedor y Producto (Top 10)")
plt.show()

"""

Realic√© una muestra de 10 ejemplos debido a la alta cantidad de vendedores.

-Se puede ver claramente que el plan m√°s vendido es el de 3GB en todos los casos.

-La vendedora Lourdes Alesci destaca vendiendo la mayor cantidad de planes de 5GB

-Se vende una m√≠nima cantidad de fibra y tv y adem√°s algunos vendedores no venden ninguno de estos productos."""

# Seleccionar solo las columnas num√©ricas
numeric_cols = df_clear.select_dtypes(include=['float64', 'int64']).columns

# Calcular la matriz de correlaci√≥n
corr_matrix = df_clear[numeric_cols].corr()

# Visualizar la matriz de correlaci√≥n con un mapa de calor
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)
plt.title('Matriz de Correlaci√≥n de Variables Num√©ricas')
plt.show()

""""C√≥digo de √Årea" y "Prefijo" tienen correlaci√≥n perfecta ya que el c√≥digo de √°rea y prefijo telef√≥nico est√°n directamente relacionados, por lo tanto,  una de estas variables podr√≠a eliminarse del an√°lisis as√≠ no es redundante.

Se puede analizar si hay un patr√≥n en los "D√≠as transcurridos desde" y "Confirmaci√≥n SIM" para optimizar los tiempos de espera.

**Conclusiones del EDA**

Se observa que la mayor√≠a de las ventas est√°n concentradas en ciertos productos espec√≠ficos (ej. Plan 3GB).

  Hay una relaci√≥n entre cantidad de ventas y tasa de activaci√≥n, donde algunos vendedores convierten m√°s ventas en activaciones exitosas.

Recomendaci√≥n:
 Diversificar la oferta de productos ‚Üí Incentivar ventas de planes menos vendidos (como fibra y TV).

 Diferenciar estrategias para clientes corporativos ‚Üí Su proceso de compra es diferente al de clientes individuales.

# **Preprocesamiento de datos**

Codificaci√≥n de variables categ√≥ricas
"""

le = LabelEncoder()
df_clear['Vendedor_encoded'] = le.fit_transform(df_clear['Vendedor'])

df_clear['Vendedor_encoded']

"""A trav√©s del LabelEncoding convierto la variable "Vendedor" en una variable num√©rica




"""

# Paso 1: Generar columnas OneHotEncoding a partir de 'Supervisor' (usando el df original)
supervisor_ohe = pd.get_dummies(df['Supervisor'], prefix='Supervisor')

# Paso 2: Agregar las columnas nuevas al dataframe limpio df_clear
df_clear = pd.concat([df_clear, supervisor_ohe], axis=1)

df_clear.head()

"""Converti la variable "Supervisor" en una variable num√©rica con OneHotEncoding"""

# Agregar la columna 'Edad'
df_clear['Fecha de Venta'] = pd.to_datetime(df['Fecha de Venta'], errors='coerce', dayfirst=True)
df_clear['Fecha de Nacimiento'] = pd.to_datetime(df['Fecha de Nacimiento'], errors='coerce', dayfirst=True)
df_clear['Edad'] = df_clear.apply(
    lambda row: row['Fecha de Venta'].year - row['Fecha de Nacimiento'].year
    if pd.notnull(row['Fecha de Venta']) and pd.notnull(row['Fecha de Nacimiento']) else None, axis=1
)
df_clear['Edad'] = df_clear['Edad'].fillna(df_clear['Edad'].median())

# Escalar con StandardScaler
scaler_std = StandardScaler()
df_clear['Edad'] = scaler_std.fit_transform(df_clear[['Edad']])

# Escalar con RobustScaler
scaler_robust = RobustScaler()
df_clear[['Cantidad de portas', 'D√≠as transcurridos desde']] = scaler_robust.fit_transform(
    df_clear[['Cantidad de portas', 'D√≠as transcurridos desde']]
)


# --- Escalado espec√≠fico ---
# Variables originales
original = df_clear[['Edad', 'Cantidad de portas', 'D√≠as transcurridos desde']].copy()

# Aplicar escaladores
edad_std = StandardScaler().fit_transform(df_clear[['Edad']])
otras_robust = RobustScaler().fit_transform(df_clear[['Cantidad de portas', 'D√≠as transcurridos desde']])

# Combinar en nuevo DataFrame escalado
escalado = pd.DataFrame({
    'Edad': edad_std.flatten(),
    'Cantidad de portas': otras_robust[:, 0],
    'D√≠as transcurridos desde': otras_robust[:, 1]
})

# --- Gr√°fico comparativo ---
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))
variables = ['Edad', 'Cantidad de portas', 'D√≠as transcurridos desde']

for i, var in enumerate(variables):
    # Histograma original
    axes[i, 0].hist(original[var], bins=30, color='lightgray', edgecolor='black')
    axes[i, 0].set_title(f'{var} - Original')
    axes[i, 0].set_ylabel('Frecuencia')

    # Histograma escalado
    axes[i, 1].hist(escalado[var], bins=30, color='skyblue', edgecolor='black')
    axes[i, 1].set_title(f'{var} - Escalado')

plt.tight_layout()
plt.show()

"""Luego de probar ambos modelos, elijo escalar la variable 'Edad' con Stardard Scaler y 'Cantidad de Portas' con 'Dias transcurridos desde' aplicando Robust Scaler

Esto se debe a que entiendo que al tener outliers en el caso de 'Cantidad de portas' y distribucion sesgada en el caso de  'Dias transcurridos desde' , necesitan un modelo m√°s robusto que no distorsione el resultado

Edad = Diferencias grandes entre edades que pueden afectar al modelo

Cantidad de portas = Tiene valores peque√±os pero con outliers

D√≠as transcurridos desde  = Varia bastante su comportamiento

# **Feature Selection**

Selecci√≥n de variable objetivo y variables independientes
"""

# Estandarizar texto en min√∫sculas y quitar espacios extra
df['Estado de Solicitud'] = df['Estado de Solicitud'].str.strip().str.lower()

# Crear columna binaria: 1 si es 'aprobada', 0 en cualquier otro caso
df_clear['Estado_binario'] = df['Estado de Solicitud'].apply(lambda x: 1 if x == 'aprobada' else 0)

df_clear['Estado_binario'].value_counts(normalize=True)

"""Transformo mi variable ‚ÄòEstado de Solicitud‚Äô en una variable binaria llamada ‚ÄòEstado binario‚Äô para poder analizar su correlaci√≥n"""

print(df_clear.select_dtypes(include=['int64', 'float64']).columns)

"""Reviso que la considere de tipo num√©rica"""

# Seleccionar solo columnas num√©ricas
numericas = df_clear.select_dtypes(include=['int64', 'float64'])

# Calcular matriz de correlaci√≥n
correlacion = numericas.corr()

# Graficar heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlacion, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5)
plt.title("üìä Matriz de Correlaci√≥n de Variables Num√©ricas")
plt.tight_layout()
plt.show()

"""Puedo observar que:


'Dias trancurridos desde': 	A mayor cantidad de d√≠as, m√°s probabilidad de aprobaci√≥n (¬øposiblemente por ventas corporativas o validadas?).

'Edad': Ligera correlaci√≥n: edad m√°s alta, mayor aprobaci√≥n.

'Cantidad de portas': A m√°s portas, mayor chance de aprobaci√≥n (quiz√° ventas grupales con validaci√≥n m√°s fuerte).

# **Modelos**

**Librerias necesarias para implementar los modelos**

Utilizo Random Forest Classifier
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

"""**Divisi√≥n de datos en conjuntos de entrenamiento y prueba**"""

variables_utiles = [
    'Edad',
    'Cantidad de portas',
    'D√≠as transcurridos desde',
]

# Variables predictoras y objetivo
X = df_clear[variables_utiles]
y = df_clear['Estado_binario']

# Divisi√≥n de datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Modelo Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

"""**Predicci√≥n con conjunto de prueba**"""

y_pred = rf_model.predict(X_test)

"""**Evaluaci√≥n del rendimiento del modelo**"""

print("üìä Matriz de Confusi√≥n:")
print(confusion_matrix(y_test, y_pred))

print("\nüìã Reporte de Clasificaci√≥n:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
labels = ['No Aprobada', 'Aprobada']

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.title('Matriz de Confusi√≥n')
plt.xlabel('Predicci√≥n')
plt.ylabel('Valor Real')
plt.show()

"""# **Utilizo  XGBoost**

**Librerias necesarias para implementar los modelos**
"""

!pip install xgboost
import xgboost as xgb

"""**Divisi√≥n de datos en conjuntos de entrenamiento y prueba**"""

# 1. Cargar el modelo
xgb_model = xgb.XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# 2. Entrenar el modelo con tus datos
xgb_model.fit(X_train, y_train)

"""**Predicci√≥n con conjunto de prueba**"""

y_pred_xgb = xgb_model.predict(X_test)

"""**Evaluaci√≥n del rendimiento del modelo**"""

print("üìä Matriz de Confusi√≥n - XGBoost:")
print(confusion_matrix(y_test, y_pred_xgb))

print("\nüìã Reporte de Clasificaci√≥n - XGBoost:")
print(classification_report(y_test, y_pred_xgb))

cm = confusion_matrix(y_test, y_pred_xgb)
labels = ['No Aprobada', 'Aprobada']

# Graficar la matriz
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.title("üìä Matriz de Confusi√≥n - XGBoost")
plt.xlabel("Predicci√≥n")
plt.ylabel("Valor Real")
plt.tight_layout()
plt.show()

"""**Conclusi√≥n sobre el modelado y las metricas**

- Random Forest gana en:

Precisi√≥n general (mejor accuracy, precision, recall, y f1 en ambas clases)

Predice mejor ambas clases sin perder mucho balance

Mejora ligeramente la recuperaci√≥n de aprobadas (Clase 1)


- XGBoost est√° cerca, pero:

Tiene un poquito m√°s de falsos negativos y falsos positivos

Es m√°s sensible al ajuste de hiperpar√°metros (lo podemos mejorar)

Puede rendir mejor con m√°s datos o m√°s features

# **Optimizaci√≥n de modelos**
"""

from sklearn.model_selection import GridSearchCV
# Crear el modelo base
xgb_clf = xgb.XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# Definir el espacio de b√∫squeda
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8],
    'colsample_bytree': [0.8]
}

# Configurar GridSearchCV
grid_search = GridSearchCV(
    estimator=xgb_clf,
    param_grid=param_grid,
    scoring='f1',
    cv=3,
    verbose=1,
    n_jobs=-1
)

# Ejecutar b√∫squeda
grid_search.fit(X_train, y_train)

# Ver mejores par√°metros
print("üîß Mejores hiperpar√°metros encontrados:")
print(grid_search.best_params_)

# Entrenar modelo final con esos par√°metros
best_xgb = grid_search.best_estimator_
y_pred_best = best_xgb.predict(X_test)

# Evaluaci√≥n
from sklearn.metrics import confusion_matrix, classification_report
print("\nüìä Matriz de Confusi√≥n:")
print(confusion_matrix(y_test, y_pred_best))
print("\nüìã Reporte de Clasificaci√≥n:")
print(classification_report(y_test, y_pred_best))

"""**Conclusi√≥n sobre la optimizaci√≥n**

Recupera el 96% de las aprobadas ‚Üí si el objetivo es detectar todas las oportunidades de venta, este modelo es excelente.

Tiene un F1-score muy alto en clase 1 (aprobada), ideal si el foco est√° en no perder clientes listos para cerrar.

Alt√≠simo n√∫mero de falsos positivos: 116 ventas no aprobadas fueron clasificadas como si lo fueran.

Recall de la clase 0 cae a 0.24 ‚Üí el modelo casi no aprende a decir ‚Äúesto va a ser rechazado‚Äù.
"""

# 1. Obtener probabilidades de clase 1
y_proba = best_xgb.predict_proba(X_test)[:, 1]

# 2. Aplicar nuevo umbral de decisi√≥n
y_pred_thresh = (y_proba > 0.6).astype(int)

# 3. Matriz de confusi√≥n
cm = confusion_matrix(y_test, y_pred_thresh)
labels = ['No Aprobada', 'Aprobada']

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.title("üìä Matriz de Confusi√≥n - XGBoost con Umbral 0.6")
plt.xlabel("Predicci√≥n")
plt.ylabel("Valor Real")
plt.tight_layout()
plt.show()

# 4. Reporte de m√©tricas
print("\nüìã Reporte de Clasificaci√≥n - Umbral 0.6:")
print(classification_report(y_test, y_pred_thresh))

"""Para sacarme la duda, ajuste un umbral a 0.6 y me dio como resultado:

Alta precisi√≥n y recall en aprobadas ‚Üí no perd√©s tantas ventas

Mejor recuperaci√≥n de las no aprobadas que con 0.5

F1-score muy s√≥lido (0.76) para aprobadas

Mejor accuracy global (70%)

Es decir, es el m√°s equilibrado.

# **Conclusiones Finales**

Luego de desarrollar un modelo de machine learning capaz de predecir si una venta de portabilidad realizada por un call center ser√° aprobada o no, utilizando datos hist√≥ricos de clientes, producto, gesti√≥n comercial y caracter√≠sticas de los operadores.

Se construyeron y compararon distintos modelos (Random Forest y XGBoost), con un enfoque especial en:

Limpieza y escalado de datos inteligente (usando StandardScaler para Edad y RobustScaler para las variables con outliers)

Ingenier√≠a de variables relevantes seg√∫n correlaci√≥n con el resultado

Codificaci√≥n adecuada de variables categ√≥ricas (Supervisor, Vendedor)

Ajuste fino del umbral de clasificaci√≥n, lo que permiti√≥ encontrar el mejor equilibrio entre recall y precisi√≥n

Modelo final seleccionado: XGBoost optimizado

Con umbral ajustado a 0.6 para maximizar balance entre predicciones acertadas y reducci√≥n de erroresv

Resultado:
Detecta m√°s del 76% de las ventas aprobadas

Reduce significativamente los falsos positivos comparado con el umbral por defecto

Ofrece un equilibrio realista para escenarios comerciales donde tanto la efectividad de ventas como la eficiencia operativa son importantes

El modelo desarrollado es robusto, balanceado y est√° alineado con los objetivos del negocio. Permite anticiparse al resultado de las ventas con una precisi√≥n s√≥lida, brindando una herramienta valiosa para la gesti√≥n predictiva del rendimiento en campa√±as de portabilidad.
"""