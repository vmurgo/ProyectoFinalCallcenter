# -*- coding: utf-8 -*-
"""Proyecto_Pre_Entrega_VentasEnero25.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ve06XMv_kN-BCZS8XZG3ccmXtT6YdyNs

# **Presentación del proyecto**

**Abstracto con Motivación y Audiencia**

>




Este proyecto tiene como objetivo analizar y visualizar el desempeño de las ventas realizadas por un Call Center de Movistar durante el mes de enero de 2025. A través de la exploración de datos, buscamos identificar tendencias en las ventas, los productos más solicitados y la tasa de activaciones.

Se utilizarán técnicas de limpieza de datos, análisis exploratorio y visualización para extraer información valiosa que pueda ayudar en la toma de decisiones estratégicas.

Este análisis está dirigido a diferentes actores clave dentro de la empresa, incluyendo:

- Gerencia Comercial → Para identificar oportunidades de crecimiento y optimizar estrategias de venta.
- Supervisores del Call Center → Para evaluar el rendimiento del equipo y mejorar los procesos de atención.
- Analistas de Datos → Para generar reportes estratégicos.
- Equipo de Retención y Fidelización → Para detectar clientes potenciales y reducir la tasa de abandono.

**Contexto Comercial y Analítico**

El sector de telecomunicaciones es altamente competitivo, y las estrategias de ventas a través de call centers juegan un papel fundamental en la adquisición de clientes, fidelización y rentabilidad de las empresas.

**Preguntas/Hipótesis a Resolver mediante el Análisis de Datos**

- ¿Cuántas ventas se realizaron en enero de 2025?
- ¿Cuál es la tasa de activación de las ventas realizadas?
- ¿Cuáles fueron los productos más vendidos?

Hipótesis:

 - La mayoría de las ventas corresponden a portabilidades en lugar de nuevas líneas
 - El producto más vendido es el más económico

**Objetivo**

# **Lectura de datos**

**Librerias necesarias para el analisis de datos**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
!pip install xgboost
import xgboost as xgb
from sklearn.model_selection import GridSearchCV

"""**Configuración del entorno de Python en Google Colab**"""

from google.colab import drive
import os

drive.mount("/content/drive")
print(os.getcwd())

os.chdir("/content/drive/My Drive/")
print(os.getcwd())

"""**Lectura del dataset**"""

df = pd.read_csv('/content/drive/MyDrive/DataFrameVentas.csv', sep=";", encoding="latin1")

"""**Análisis inicial del dataset**"""

df.shape

"""Indica que mi dataset contiene 91 columnas y 2035 filas"""

df.head()

df.info()

"""**Descripción de variables**

Id: Número de id de la carga de venta.

Fecha de venta: fecha en la que se carga la venta

Hora de carga: hora en la que se carga la venta

Fecha de Activación: fecha en la que se activa la portabilidad

Linea a portar: número de teléfono del cliente

Bandeja IRIS: sistema donde fue cargado

Fecha de portación: fecha de portabilidad asignada

Fecha de aprobación: fecha de aprobacion asignada

Usuario de la linea: número del titular de la línea

Producto anterior: tipo de producto anterior

Plan actual:

Producto: producto que adquiere en esta compañia

Precio:

Cantidad de portas: cantidad de números del cliente que realizan la portabilidad

Estado de Solicitud: estado administrativo en el que se encuentra el trámite

Subestado: para alguna aclaración necesaria

Motivo de Rechazo: si la solicitud fuese rechazada, se agregan los motivos

Fecha de rechazo: se agrega fecha de dicho rechazo

Motivo de Estado: para información adicional

PO: Número de PO que asigna Movistar

Nombre y Apellido del titula: Aparece solo el nombre del titular

Apellido: indica el apellido del titular

DNI: número de identificación del titular

Vendedor: agente que realizó la venta

ID secundario: ID del agente

Ejecutivo: entidad que realizó la venta

Supervisor: lider del agente

Envio: medio por el que se realiza el envio

Número de Guia: número de seguimiento

Número de factura:

Fuente de Solicitud: fuente donde recibe el dato el vendedor

Número de Orden: aclaración entre los administrativos

Observaciones: celda para agregar aclaraciones

Provincia: Provincia del envio

Ciudad: Ciudad del envio

Código Postal: CP del envio

Fecha de Nacimiento: fecha de nacimiento del titular

Dirección: dirección del envio

Número: número de la dirección del envio

Torre/Monoblock: Torre del envio

Piso: Piso del envio

Departamento: Departamento del envio

Entre Calles: Entre calles del envio

Barrio: Barrio del envio

Manzana: Manzana del envio

Casa/Lote: Casa del envio

Otras referencias: Referencias adicionales

Referencias: Referencias adicionales

Teléfono Principal: Teléfono principal del titular

Teléfono Adicional: Teléfono Adicional del titular

Teléfono Adicional 2: Teléfono Adicional conviviente del titular

Numero de Chip: Número asignado de CHIP

Código de area: Código de área del titular

Prefijo: Prefijo del número del titular

Compañia de Teléfono: compañia de teléfono anterior

Fecha de Cita: fecha retiro en sucursal

Hora de Cita: hora retiro en sucursal

Comentarios: comentarios adicionales

Número de PIN: código de portabilidad asignado

CHIP en mano: indica si el chip fue recibido

Totalización: si agrega FTTH

Fecha de Envio de CHIP: fecha del envio

Fecha entrega SIM: fecha de la entrega

Tipo de confirmación SIM: como confirman que llego el chip

Fecha llamado confirmación SIM: fecha de confirmación

Fecha de Vencimiento de Pin: vencimiento del código de portabilidad asignado

Usuario logística: quien realiza el envio

Comentarios de logística: comentarios adicionales

Días transcurridos desde: días transcurridos desde la venta

Fecha de cambio de estado: fecha de último cambio de estado

ANI fijo asignado: Número de fijo asignado para FTTH

centralizador_ftth_latitud : latitud

centralizador_ftth_longitud longuitud

centralizador_ftth_ctos_cercanos: cantidad de CTOS cercanos

centralizador_fecha_sincronizacion: fecha de centralizado

Formulario de calidad completado: indica si completa formulario

Formulario de calidad exitoso : indica si el formulario esta bien

Línea a portar correcta: campo adicional en el caso de error de carga

Titular de linea correcto : campo adicional en el caso de error de carga

Producto actual correcto: campo adicional en el caso de error de carga

Compañía actual correcta: campo adicional en el caso de error de carga

Documento correcto: campo adicional en el caso de error de carga

Domicilio correcta: campo adicional en el caso de error de carga

Producto correcto : campo adicional en el caso de error de carga

Informacion correcta: campo adicional en el caso de error de carga

Email calidad: email de cliente

Interesado en fibra: indica si compró fibra o no

Dirección de fibra: direccion de instalación de fibra

Link Google Maps: link de la dirección en google maps
"""

df.describe()

"""# **Data Wranglimg - Limpieza y transformación de datos**

**Valores duplicados**
"""

df.duplicated().sum()

"""Identifico que no tengo ningun valor duplicado.

**Valores nulos**
"""

df.isnull().sum()

"""Reviso la cantidad de valores nulos y en que columnas se encuentran."""

df_clear = df.dropna(axis=1, how='all')

"""Después de revisar los valores, decido eliminar únicamente las columnas que presentan la totalidad de sus valores nulos."""

columnas_a_eliminar = [
    "Id",
    "Formulario de calidad completado", "Línea a portar correcta",
    "Titular de linea correcto", "Producto actual correcto",
    "Compañía actual correcta", "Documento correcto",
    "Domicilio correcta", "Producto correcto",
    "Email calidad", "Interesado en fibra",
    "Dirección de fibra","Link Google Maps",
    "Fecha de envío de Chip", "Fecha entrega SIM",
    "Tipo de confirmación SIM","Fecha de Vencimiento de Pin",
    "Usuario logística","Comentarios de logística",
    "Fecha de cambio de estado","ANI fijo asignado",
    "Fecha de cita", "Hora de cita",
    "Teléfono Principal (Fijo)",
    "Teléfono Adicional (Celular)",
    "Teléfono Adicional #2 (Celular)",
    "Dirección", "Número",
    "Torre / Monoblock","Piso",
    "Departamento","Entre Calles",
    "Barrio","Manzana",
    "Casa / Lote", "Otras referencias",
    "Referencias","Nexo",
    "Fecha de Nacimiento", "Envío",
    "Número de Guía","Número de Orden",
    "Observaciones", "Comentarios",
    "Precio"
]

df_clear = df.drop(columns=columnas_a_eliminar, errors='ignore')

"""Elimino todas las columnas que interpreto que no son útiles para analizar y que entorpecen el análisis de mi dataset."""

print(df_clear.info())

"""**Tratamiento de valores outliers**"""

df_clear["Producto"] = df_clear["Producto"].replace({
    "C. 1.5 GB FULL $19.150 - BONO DE DESCUENTO POR 12 MESES DE $15.700-": "C.Plan 1.5GB",
    "C. 10 GB FULL $57.750 - BONO DE DESCUENTO POR 12 MESES DE $48.900-": "C.Plan 10GB",
    "C. 15 GB FULL $63.950 - BONO DE DESCUENTO POR 12 MESES DE $52.450-": "C.Plan 15GB",
    "C. 25 GB FULL $72.700 - BONO DE DESCUENTO POR 12 MESES DE $59.450-": "C.Plan 25GB",
    "C. 3GB PROMO EXCLUSIVA $20.900 - BONO DE DESCUENTO POR 24 MESES DE $17.800-": "C.Plan 3GB",
    "C. 6GB FULL PROMO EXCLUSIVA $29.800 - BONO DE DESCUENTO POR 24 MESES DE $25.300-": "C.Plan 6GB",
    "FIBRA PLAN MOVISTAR 100 MB": "FIBRA 100MB",
    "FIBRA PLAN MOVISTAR 300 MB": "FIBRA 300MB",
    "FIBRA PLAN MOVISTAR 300 MB + TV DIGITAL": "FIBRA 300MB + TV",
    "FIBRA PLAN MOVISTAR GIGA + TV DIGITAL": "FIBRA 1GB + TV",
    "P. 10 GB FULL $57.750 - BONO DE DESCUENTO POR 12 MESES DE $16.750-": "P.Plan 10GB-",
    "P. 3 GB FULL $25.550 - BONO DE DESCUENTO POR 12 MESES DE $6.550-": "P.Plan 3GB-",
    "C. 3 GB FULL $25.550 - BONO DE DESCUENTO POR 12 MESES DE $21.050-": "C.Plan 3GB",
    "C. 6 GB FULL $29.800 - BONO DE DESCUENTO POR 12 MESES DE $25.300-": "C.Plan 6GB"

})

df_clear["Producto"].value_counts().plot(kind="bar", figsize=(12,6), title="Ventas por Producto")
plt.xlabel("Producto")
plt.ylabel("Cantidad de Ventas")
plt.xticks(rotation=45)
plt.show()

"""El valor del outlier se encuentra en el plan de 3GB, pero decido no eliminarlo o ajustarlo con la moda, ya que representa un caso real y significativo: ventas de productos altamente demandados."""

df_clear[df_clear["Producto"].str.contains("3GB", na=False)]["Estado de Solicitud"].value_counts()

"""Reviso su estado administrativo para saber si realmente son ventas aprobadas. De estas han sido aprobadas 1113 ventas de porta y 4 de corpo. Si bien hay un número significativo de ventas en curso o canceladas, considero que este outlier no debe ser eliminado porque representa que es la promo más comprada."""

ventas_por_producto = df_clear["Producto"].value_counts()
plt.figure(figsize=(12,6))
ventas_por_producto.plot(kind="bar", color=['red' if "3GB" in p else 'royalblue' for p in ventas_por_producto.index])
plt.xticks(rotation=45)
plt.xlabel("Producto")
plt.ylabel("Cantidad de Ventas")
plt.title("Distribución de Ventas por Producto (Resaltando el Plan 3GB)")
plt.show()

"""Destaco con un color el outlier para marcar el plan más vendido

**Otras transformaciones, normalización, fechas**

# **Análisis Exploratorio de datos**

**Análisis univariado**
"""

plt.figure(figsize=(8,5))
sns.boxplot(x=df["Cantidad de portas"], color="lightblue")
plt.title("Boxplot de Cantidad de Portas")
plt.show()

"""La mayor parte de los datos se encuentran entre  1 y 3 portas vendidas y su mediana es de 2 porta. El rango intercuartílico indica que la mayoria se encuentran en un rango bajo. Se obsevan varios outliers mayores a 3, pero uno que destaca de 15."""

df_clear[df_clear["Cantidad de portas"] == 15]["Estado de Solicitud"]

"""El outlier no es un error, sino que corresponde a clientes corporativos que portan múltiples líneas en una sola transacción. Esto explica el valor atípico en la variable "Cantidad de portas".

**Análisis bivariado**

Relación entre Tiempo de Activación y Cantidad de Portas
"""

sns.scatterplot(x=df_clear["Días transcurridos desde"], y=df_clear["Cantidad de portas"])
plt.title("Relación entre Tiempo de Activación y Cantidad de Portas")
plt.show()

"""La activación suele completarse en períodos de 0 a 10 días, con algunos casos extendiéndose hasta los 25 días.

Nuevamente aparecen las 15 portas como outlier, pero esta vez en tiempo de activación, lo que sugiere que los clientes corporativos tardan más en activar que la porta individuos. Para los demás casos, no hay un patrón claro que indique que más portas = mayor tiempo de activación.

**Análisis multivariado**
"""

pivot_table = df_clear.pivot_table(index="Vendedor", columns="Producto", values="Cantidad de portas", aggfunc="sum")

pivot_table_top10 = pivot_table.head(10)

plt.figure(figsize=(12,6))
sns.heatmap(pivot_table_top10, cmap="coolwarm", annot=True, fmt=".0f")
plt.title("Matriz de Ventas por Vendedor y Producto (Top 10)")
plt.show()

"""

Realicé una muestra de 10 ejemplos debido a la alta cantidad de vendedores.

-Se puede ver claramente que el plan más vendido es el de 3GB en todos los casos.

-La vendedora Lourdes Alesci destaca vendiendo la mayor cantidad de planes de 5GB

-Se vende una mínima cantidad de fibra y tv y además algunos vendedores no venden ninguno de estos productos."""

# Seleccionar solo las columnas numéricas
numeric_cols = df_clear.select_dtypes(include=['float64', 'int64']).columns

# Calcular la matriz de correlación
corr_matrix = df_clear[numeric_cols].corr()

# Visualizar la matriz de correlación con un mapa de calor
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True)
plt.title('Matriz de Correlación de Variables Numéricas')
plt.show()

""""Código de Área" y "Prefijo" tienen correlación perfecta ya que el código de área y prefijo telefónico están directamente relacionados, por lo tanto,  una de estas variables podría eliminarse del análisis así no es redundante.

Se puede analizar si hay un patrón en los "Días transcurridos desde" y "Confirmación SIM" para optimizar los tiempos de espera.

**Conclusiones del EDA**

Se observa que la mayoría de las ventas están concentradas en ciertos productos específicos (ej. Plan 3GB).

  Hay una relación entre cantidad de ventas y tasa de activación, donde algunos vendedores convierten más ventas en activaciones exitosas.

Recomendación:
 Diversificar la oferta de productos → Incentivar ventas de planes menos vendidos (como fibra y TV).

 Diferenciar estrategias para clientes corporativos → Su proceso de compra es diferente al de clientes individuales.

# **Preprocesamiento de datos**

Codificación de variables categóricas
"""

le = LabelEncoder()
df_clear['Vendedor_encoded'] = le.fit_transform(df_clear['Vendedor'])

df_clear['Vendedor_encoded']

"""A través del LabelEncoding convierto la variable "Vendedor" en una variable numérica




"""

# Paso 1: Generar columnas OneHotEncoding a partir de 'Supervisor' (usando el df original)
supervisor_ohe = pd.get_dummies(df['Supervisor'], prefix='Supervisor')

# Paso 2: Agregar las columnas nuevas al dataframe limpio df_clear
df_clear = pd.concat([df_clear, supervisor_ohe], axis=1)

df_clear.head()

"""Converti la variable "Supervisor" en una variable numérica con OneHotEncoding"""

# Agregar la columna 'Edad'
df_clear['Fecha de Venta'] = pd.to_datetime(df['Fecha de Venta'], errors='coerce', dayfirst=True)
df_clear['Fecha de Nacimiento'] = pd.to_datetime(df['Fecha de Nacimiento'], errors='coerce', dayfirst=True)
df_clear['Edad'] = df_clear.apply(
    lambda row: row['Fecha de Venta'].year - row['Fecha de Nacimiento'].year
    if pd.notnull(row['Fecha de Venta']) and pd.notnull(row['Fecha de Nacimiento']) else None, axis=1
)
df_clear['Edad'] = df_clear['Edad'].fillna(df_clear['Edad'].median())

# Escalar con StandardScaler
scaler_std = StandardScaler()
df_clear['Edad'] = scaler_std.fit_transform(df_clear[['Edad']])

# Escalar con RobustScaler
scaler_robust = RobustScaler()
df_clear[['Cantidad de portas', 'Días transcurridos desde']] = scaler_robust.fit_transform(
    df_clear[['Cantidad de portas', 'Días transcurridos desde']]
)


# --- Escalado específico ---
# Variables originales
original = df_clear[['Edad', 'Cantidad de portas', 'Días transcurridos desde']].copy()

# Aplicar escaladores
edad_std = StandardScaler().fit_transform(df_clear[['Edad']])
otras_robust = RobustScaler().fit_transform(df_clear[['Cantidad de portas', 'Días transcurridos desde']])

# Combinar en nuevo DataFrame escalado
escalado = pd.DataFrame({
    'Edad': edad_std.flatten(),
    'Cantidad de portas': otras_robust[:, 0],
    'Días transcurridos desde': otras_robust[:, 1]
})

# --- Gráfico comparativo ---
fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(12, 10))
variables = ['Edad', 'Cantidad de portas', 'Días transcurridos desde']

for i, var in enumerate(variables):
    # Histograma original
    axes[i, 0].hist(original[var], bins=30, color='lightgray', edgecolor='black')
    axes[i, 0].set_title(f'{var} - Original')
    axes[i, 0].set_ylabel('Frecuencia')

    # Histograma escalado
    axes[i, 1].hist(escalado[var], bins=30, color='skyblue', edgecolor='black')
    axes[i, 1].set_title(f'{var} - Escalado')

plt.tight_layout()
plt.show()

"""Luego de probar ambos modelos, elijo escalar la variable 'Edad' con Stardard Scaler y 'Cantidad de Portas' con 'Dias transcurridos desde' aplicando Robust Scaler

Esto se debe a que entiendo que al tener outliers en el caso de 'Cantidad de portas' y distribucion sesgada en el caso de  'Dias transcurridos desde' , necesitan un modelo más robusto que no distorsione el resultado

Edad = Diferencias grandes entre edades que pueden afectar al modelo

Cantidad de portas = Tiene valores pequeños pero con outliers

Días transcurridos desde  = Varia bastante su comportamiento

# **Feature Selection**

Selección de variable objetivo y variables independientes
"""

# Estandarizar texto en minúsculas y quitar espacios extra
df['Estado de Solicitud'] = df['Estado de Solicitud'].str.strip().str.lower()

# Crear columna binaria: 1 si es 'aprobada', 0 en cualquier otro caso
df_clear['Estado_binario'] = df['Estado de Solicitud'].apply(lambda x: 1 if x == 'aprobada' else 0)

df_clear['Estado_binario'].value_counts(normalize=True)

"""Transformo mi variable ‘Estado de Solicitud’ en una variable binaria llamada ‘Estado binario’ para poder analizar su correlación"""

print(df_clear.select_dtypes(include=['int64', 'float64']).columns)

"""Reviso que la considere de tipo numérica"""

# Seleccionar solo columnas numéricas
numericas = df_clear.select_dtypes(include=['int64', 'float64'])

# Calcular matriz de correlación
correlacion = numericas.corr()

# Graficar heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlacion, annot=True, fmt=".2f", cmap='coolwarm', linewidths=0.5)
plt.title("📊 Matriz de Correlación de Variables Numéricas")
plt.tight_layout()
plt.show()

"""Puedo observar que:


'Dias trancurridos desde': 	A mayor cantidad de días, más probabilidad de aprobación (¿posiblemente por ventas corporativas o validadas?).

'Edad': Ligera correlación: edad más alta, mayor aprobación.

'Cantidad de portas': A más portas, mayor chance de aprobación (quizá ventas grupales con validación más fuerte).

# **Modelos**

**Librerias necesarias para implementar los modelos**

Utilizo Random Forest Classifier
"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report

"""**División de datos en conjuntos de entrenamiento y prueba**"""

variables_utiles = [
    'Edad',
    'Cantidad de portas',
    'Días transcurridos desde',
]

# Variables predictoras y objetivo
X = df_clear[variables_utiles]
y = df_clear['Estado_binario']

# División de datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Modelo Random Forest
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model.fit(X_train, y_train)

"""**Predicción con conjunto de prueba**"""

y_pred = rf_model.predict(X_test)

"""**Evaluación del rendimiento del modelo**"""

print("📊 Matriz de Confusión:")
print(confusion_matrix(y_test, y_pred))

print("\n📋 Reporte de Clasificación:")
print(classification_report(y_test, y_pred))

cm = confusion_matrix(y_test, y_pred)
labels = ['No Aprobada', 'Aprobada']

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.title('Matriz de Confusión')
plt.xlabel('Predicción')
plt.ylabel('Valor Real')
plt.show()

"""# **Utilizo  XGBoost**

**Librerias necesarias para implementar los modelos**
"""

!pip install xgboost
import xgboost as xgb

"""**División de datos en conjuntos de entrenamiento y prueba**"""

# 1. Cargar el modelo
xgb_model = xgb.XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# 2. Entrenar el modelo con tus datos
xgb_model.fit(X_train, y_train)

"""**Predicción con conjunto de prueba**"""

y_pred_xgb = xgb_model.predict(X_test)

"""**Evaluación del rendimiento del modelo**"""

print("📊 Matriz de Confusión - XGBoost:")
print(confusion_matrix(y_test, y_pred_xgb))

print("\n📋 Reporte de Clasificación - XGBoost:")
print(classification_report(y_test, y_pred_xgb))

cm = confusion_matrix(y_test, y_pred_xgb)
labels = ['No Aprobada', 'Aprobada']

# Graficar la matriz
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.title("📊 Matriz de Confusión - XGBoost")
plt.xlabel("Predicción")
plt.ylabel("Valor Real")
plt.tight_layout()
plt.show()

"""**Conclusión sobre el modelado y las metricas**

- Random Forest gana en:

Precisión general (mejor accuracy, precision, recall, y f1 en ambas clases)

Predice mejor ambas clases sin perder mucho balance

Mejora ligeramente la recuperación de aprobadas (Clase 1)


- XGBoost está cerca, pero:

Tiene un poquito más de falsos negativos y falsos positivos

Es más sensible al ajuste de hiperparámetros (lo podemos mejorar)

Puede rendir mejor con más datos o más features

# **Optimización de modelos**
"""

from sklearn.model_selection import GridSearchCV
# Crear el modelo base
xgb_clf = xgb.XGBClassifier(
    use_label_encoder=False,
    eval_metric='logloss',
    random_state=42
)

# Definir el espacio de búsqueda
param_grid = {
    'n_estimators': [50, 100],
    'max_depth': [3, 5],
    'learning_rate': [0.01, 0.1],
    'subsample': [0.8],
    'colsample_bytree': [0.8]
}

# Configurar GridSearchCV
grid_search = GridSearchCV(
    estimator=xgb_clf,
    param_grid=param_grid,
    scoring='f1',
    cv=3,
    verbose=1,
    n_jobs=-1
)

# Ejecutar búsqueda
grid_search.fit(X_train, y_train)

# Ver mejores parámetros
print("🔧 Mejores hiperparámetros encontrados:")
print(grid_search.best_params_)

# Entrenar modelo final con esos parámetros
best_xgb = grid_search.best_estimator_
y_pred_best = best_xgb.predict(X_test)

# Evaluación
from sklearn.metrics import confusion_matrix, classification_report
print("\n📊 Matriz de Confusión:")
print(confusion_matrix(y_test, y_pred_best))
print("\n📋 Reporte de Clasificación:")
print(classification_report(y_test, y_pred_best))

"""**Conclusión sobre la optimización**

Recupera el 96% de las aprobadas → si el objetivo es detectar todas las oportunidades de venta, este modelo es excelente.

Tiene un F1-score muy alto en clase 1 (aprobada), ideal si el foco está en no perder clientes listos para cerrar.

Altísimo número de falsos positivos: 116 ventas no aprobadas fueron clasificadas como si lo fueran.

Recall de la clase 0 cae a 0.24 → el modelo casi no aprende a decir “esto va a ser rechazado”.
"""

# 1. Obtener probabilidades de clase 1
y_proba = best_xgb.predict_proba(X_test)[:, 1]

# 2. Aplicar nuevo umbral de decisión
y_pred_thresh = (y_proba > 0.6).astype(int)

# 3. Matriz de confusión
cm = confusion_matrix(y_test, y_pred_thresh)
labels = ['No Aprobada', 'Aprobada']

plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.title("📊 Matriz de Confusión - XGBoost con Umbral 0.6")
plt.xlabel("Predicción")
plt.ylabel("Valor Real")
plt.tight_layout()
plt.show()

# 4. Reporte de métricas
print("\n📋 Reporte de Clasificación - Umbral 0.6:")
print(classification_report(y_test, y_pred_thresh))

"""Para sacarme la duda, ajuste un umbral a 0.6 y me dio como resultado:

Alta precisión y recall en aprobadas → no perdés tantas ventas

Mejor recuperación de las no aprobadas que con 0.5

F1-score muy sólido (0.76) para aprobadas

Mejor accuracy global (70%)

Es decir, es el más equilibrado.

# **Conclusiones Finales**

Luego de desarrollar un modelo de machine learning capaz de predecir si una venta de portabilidad realizada por un call center será aprobada o no, utilizando datos históricos de clientes, producto, gestión comercial y características de los operadores.

Se construyeron y compararon distintos modelos (Random Forest y XGBoost), con un enfoque especial en:

Limpieza y escalado de datos inteligente (usando StandardScaler para Edad y RobustScaler para las variables con outliers)

Ingeniería de variables relevantes según correlación con el resultado

Codificación adecuada de variables categóricas (Supervisor, Vendedor)

Ajuste fino del umbral de clasificación, lo que permitió encontrar el mejor equilibrio entre recall y precisión

Modelo final seleccionado: XGBoost optimizado

Con umbral ajustado a 0.6 para maximizar balance entre predicciones acertadas y reducción de erroresv

Resultado:
Detecta más del 76% de las ventas aprobadas

Reduce significativamente los falsos positivos comparado con el umbral por defecto

Ofrece un equilibrio realista para escenarios comerciales donde tanto la efectividad de ventas como la eficiencia operativa son importantes

El modelo desarrollado es robusto, balanceado y está alineado con los objetivos del negocio. Permite anticiparse al resultado de las ventas con una precisión sólida, brindando una herramienta valiosa para la gestión predictiva del rendimiento en campañas de portabilidad.
"""